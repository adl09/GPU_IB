{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adl09/GPU_IB/blob/main/.%5CClase1%5CICNPG_HolaMundo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entorno de ejecución\n",
        "\n",
        "Google colab tiene varios entornos de ejecución con distintos recursos asociados. Para usar GPUs gratuitamente, podemos elegir el entorno T4 GPU, seleccionandolo en \"Entorno de Ejecución\". Por default solo podemos usar CPU.\n",
        "\n",
        "Chequiemos luego rápidamente que tenemos acceso a una GPU de nvidia, con el siguiente comando:"
      ],
      "metadata": {
        "id": "XImsVWf8mZfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFytUBiSm2D8",
        "outputId": "e88069ac-18a4-49c6-d8f0-42e9974dc9df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Feb 10 12:47:29 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   57C    P8             12W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "el ! antes de nvidia-smi es para indicar que es un comando de linux lo que queremos correr (sino, por default todo es python).  "
      ],
      "metadata": {
        "id": "PrvzmFsI2Jal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hola Mundo"
      ],
      "metadata": {
        "id": "VOHy5NdqGRC7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En colab podemos escribir directamente codigo en python, pero también en muchos otros lenguajes no interpretados, en particular C y C++.\n",
        "\n",
        "El truco para usar colab con códigos compilables es la primer linea %%writefile para indicar que queremos que se genere un archivo con el contenido de la celda, para luego compilarlo con alguno de los compiladores preinstalados en colab."
      ],
      "metadata": {
        "id": "CFBsao5W0-1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hola.cu\n",
        "\n",
        "/*\n",
        " * Imprime \"hola mundo\" desde 10 hilos corriendo en GPU\n",
        " */\n",
        "\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "__device__ void helloFromDevice()  // <-- las funciones device son aquellas que pueden llamarse desde un kernel\n",
        "{\n",
        "    printf(\"hola mundo desde el device, hilo: %d, bloque: %d\\n\",threadIdx.x,blockIdx.x);\n",
        "}\n",
        "\n",
        "__global__ void helloFromGPU() // <-- kernel (solo puede ser llamada desde el host)\n",
        "{\n",
        "    // esto lo hace cada hilo de la grilla\n",
        "    printf(\"hola mundo desde la GPU!, hilo: %d, bloque: %d\\n\",threadIdx.x,blockIdx.x);\n",
        "    helloFromDevice();\n",
        "\n",
        "}\n",
        "\n",
        "//int main(int argc, char **argv)\n",
        "int main()\n",
        "{\n",
        "    // lanzamiento del kernel con una grilla de 1 bloque y 10 hilos por bloque\n",
        "    // es asincronico respecto de del hilo de host\n",
        "    helloFromGPU<<<2, 10>>>(); // (numero de bloques, numero de hilos por bloque)  <- conf. de la grilla\n",
        "\n",
        "    // esto lo hace la CPU independientemente de si el kernel termino o no...\n",
        "    printf(\"Hola mundo desde la CPU!\\n\\n\");\n",
        "\n",
        "    // barrera para que todo el trabajo de la gpu termine antes de seguir...\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Svad9B72xknn",
        "outputId": "047d6213-1e87-4170-d9d1-a83195c1af20"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting hola.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nvcc es el compilador de nvidia. Funciona como cualquier compilador de C/C++ si no hay partes en CUDA. Si hay código CUDA sabe como compilarlo para su ejecución en GPU."
      ],
      "metadata": {
        "id": "GK4znyY41q9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 hola.cu -o hola"
      ],
      "metadata": {
        "id": "ku8uAFCdxvD-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./hola"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjvnF_GGx60V",
        "outputId": "3e04c3ba-b696-40d5-ee81-067ee4f8e418"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hola mundo desde la CPU!\n",
            "\n",
            "hola mundo desde la GPU!, hilo: 0, bloque: 0\n",
            "hola mundo desde la GPU!, hilo: 1, bloque: 0\n",
            "hola mundo desde la GPU!, hilo: 2, bloque: 0\n",
            "hola mundo desde la GPU!, hilo: 3, bloque: 0\n",
            "hola mundo desde la GPU!, hilo: 4, bloque: 0\n",
            "hola mundo desde la GPU!, hilo: 5, bloque: 0\n",
            "hola mundo desde la GPU!, hilo: 6, bloque: 0\n",
            "hola mundo desde la GPU!, hilo: 7, bloque: 0\n",
            "hola mundo desde la GPU!, hilo: 8, bloque: 0\n",
            "hola mundo desde la GPU!, hilo: 9, bloque: 0\n",
            "hola mundo desde la GPU!, hilo: 0, bloque: 1\n",
            "hola mundo desde la GPU!, hilo: 1, bloque: 1\n",
            "hola mundo desde la GPU!, hilo: 2, bloque: 1\n",
            "hola mundo desde la GPU!, hilo: 3, bloque: 1\n",
            "hola mundo desde la GPU!, hilo: 4, bloque: 1\n",
            "hola mundo desde la GPU!, hilo: 5, bloque: 1\n",
            "hola mundo desde la GPU!, hilo: 6, bloque: 1\n",
            "hola mundo desde la GPU!, hilo: 7, bloque: 1\n",
            "hola mundo desde la GPU!, hilo: 8, bloque: 1\n",
            "hola mundo desde la GPU!, hilo: 9, bloque: 1\n",
            "hola mundo desde el device, hilo: 0, bloque: 0\n",
            "hola mundo desde el device, hilo: 1, bloque: 0\n",
            "hola mundo desde el device, hilo: 2, bloque: 0\n",
            "hola mundo desde el device, hilo: 3, bloque: 0\n",
            "hola mundo desde el device, hilo: 4, bloque: 0\n",
            "hola mundo desde el device, hilo: 5, bloque: 0\n",
            "hola mundo desde el device, hilo: 6, bloque: 0\n",
            "hola mundo desde el device, hilo: 7, bloque: 0\n",
            "hola mundo desde el device, hilo: 8, bloque: 0\n",
            "hola mundo desde el device, hilo: 9, bloque: 0\n",
            "hola mundo desde el device, hilo: 0, bloque: 1\n",
            "hola mundo desde el device, hilo: 1, bloque: 1\n",
            "hola mundo desde el device, hilo: 2, bloque: 1\n",
            "hola mundo desde el device, hilo: 3, bloque: 1\n",
            "hola mundo desde el device, hilo: 4, bloque: 1\n",
            "hola mundo desde el device, hilo: 5, bloque: 1\n",
            "hola mundo desde el device, hilo: 6, bloque: 1\n",
            "hola mundo desde el device, hilo: 7, bloque: 1\n",
            "hola mundo desde el device, hilo: 8, bloque: 1\n",
            "hola mundo desde el device, hilo: 9, bloque: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Device Query"
      ],
      "metadata": {
        "id": "8CY3_4y6Gjul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile queplacasoy.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "\tcudaDeviceProp deviceProp;\n",
        "\n",
        "\tint deviceCount = 0;\n",
        "    \tcudaError_t error_id = cudaGetDeviceCount(&deviceCount);\n",
        "\n",
        "\n",
        "\tprintf(\"En este nodo hay %d placas\\n\\n\",deviceCount);\n",
        "\tfor(int dev=0;dev<deviceCount;dev++){\n",
        "\t    \tcudaSetDevice(dev);\n",
        "    \t\tcudaGetDeviceProperties(&deviceProp, dev);\n",
        "    \t\tprintf(\"Hola!, yo soy [Device %d: \\\"%s\\\"], tu acelerador grafico personal\\n\", dev, deviceProp.name);\n",
        "\t}\n",
        "\n",
        "\tint dev; cudaGetDevice(&dev);\n",
        "\tprintf(\"\\nle asigno la device %d, que esta desocupada\\n\", dev);\n",
        "\n",
        "\treturn 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9pJUCr33FXS",
        "outputId": "f4f26747-ce2f-4ed7-c4ed-04c9c0289e06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing queplacasoy.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc queplacasoy.cu -o queplacasoy"
      ],
      "metadata": {
        "id": "AKDYKjOx3K4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./queplacasoy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5ceLoY13RvK",
        "outputId": "e9704317-889a-407a-dd9e-e1bd848337ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "En este nodo hay 1 placas\n",
            "\n",
            "Hola!, yo soy [Device 0: \"Tesla T4\"], tu acelerador grafico personal\n",
            "\n",
            "le asigno la device 0, que esta desocupada\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicios\n",
        "* Consultar la guía de programació y obtener mas información de la GPU."
      ],
      "metadata": {
        "id": "gycvtxWG6b_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hilos, bloques, grillas"
      ],
      "metadata": {
        "id": "_Dpph6eTGKW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile grillas.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "// kernel\n",
        "__global__ void Quiensoy()\n",
        "{\n",
        "    printf(\"Soy el thread (%d,%d,%d) del bloque (%d,%d,%d) [blockDim=(%d,%d,%d),gridDim=(%d,%d,%d)] \\n\",\n",
        "      threadIdx.x,threadIdx.y,threadIdx.z,blockIdx.x,blockIdx.y,blockIdx.z,\n",
        "      blockDim.x,blockDim.y,blockDim.z,gridDim.x,gridDim.y,gridDim.z);\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "\n",
        "\tif(argc!=7) {\n",
        "\t\tprintf(\"uso: %s ntbx ntby ntbz nbgx nbgy nbgz\\n\", argv[0]);\n",
        "\n",
        "\t\t// ejemplo 1\n",
        "\t\tprintf(\"ejemplo 1: Quiensoy<<< 3, 2>>> == Quiensoy<<< dim3(3,1,1), dim3(2,1,1)>>>\\n\");\n",
        "\t\tQuiensoy<<< dim3(3,1,1), dim3(2,1,1)>>>();\n",
        "\t\tcudaDeviceSynchronize();\n",
        "\n",
        "\t\t// ejemplo 2\n",
        "\t\tprintf(\"ejemplo 2: Quiensoy<<< dim3(2,2), dim3(2,1) >>>();\\n\");\n",
        "\t\tdim3 nb(2,2);\n",
        "\t\tdim3 nt(2,1);\n",
        "\t\tQuiensoy<<< nb, nt >>>();\n",
        "\t\tcudaDeviceSynchronize();\n",
        "\t}\n",
        "\telse{\n",
        "\t\tdim3 nThreads_per_block;\n",
        "\t\tdim3 nBlocks_per_grid;\n",
        "\n",
        "\t\tnThreads_per_block.x = atoi(argv[1]);\n",
        "\t\tnThreads_per_block.y = atoi(argv[2]);\n",
        "\t\tnThreads_per_block.z = atoi(argv[3]);\n",
        "\t\tnBlocks_per_grid.x = atoi(argv[4]);\n",
        "\t\tnBlocks_per_grid.y = atoi(argv[5]);\n",
        "\t\tnBlocks_per_grid.z = atoi(argv[6]);\n",
        "\n",
        "\t\t// kernel\n",
        "\t\tprintf(\"\\nDesde del host lanzamos\\n Quiensoy<<< dim3(%d,%d,%d), dim3(%d,%d,%d) >>>():\\n\\n\",\n",
        "\t\tnThreads_per_block.x,nThreads_per_block.y,nThreads_per_block.z,\n",
        "\t\tnBlocks_per_grid.x,nBlocks_per_grid.y,nBlocks_per_grid.z);\n",
        "\n",
        "\t\tprintf(\"Y los hilos imprimen:\\n\");\n",
        "\t\tQuiensoy<<< nBlocks_per_grid, nThreads_per_block >>>();\n",
        "\t\tcudaDeviceSynchronize();\n",
        "\t}\n",
        "\treturn 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDaCChZb39lu",
        "outputId": "25f56907-c7e8-499e-f6ee-6e6b2fda3a46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting grillas.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 grillas.cu -o grillas"
      ],
      "metadata": {
        "id": "BYDhPfe94C-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./grillas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0X3UAnl4Evd",
        "outputId": "bf451858-1d01-4686-8d42-297c21f9fb46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "uso: ./grillas ntbx ntby ntbz nbgx nbgy nbgz\n",
            "ejemplo 1: Quiensoy<<< 3, 2>>> == Quiensoy<<< dim3(3,1,1), dim3(2,1,1)>>>\n",
            "Soy el thread (0,0,0) del bloque (1,0,0) [blockDim=(2,1,1),gridDim=(3,1,1)] \n",
            "Soy el thread (1,0,0) del bloque (1,0,0) [blockDim=(2,1,1),gridDim=(3,1,1)] \n",
            "Soy el thread (0,0,0) del bloque (2,0,0) [blockDim=(2,1,1),gridDim=(3,1,1)] \n",
            "Soy el thread (1,0,0) del bloque (2,0,0) [blockDim=(2,1,1),gridDim=(3,1,1)] \n",
            "Soy el thread (0,0,0) del bloque (0,0,0) [blockDim=(2,1,1),gridDim=(3,1,1)] \n",
            "Soy el thread (1,0,0) del bloque (0,0,0) [blockDim=(2,1,1),gridDim=(3,1,1)] \n",
            "ejemplo 2: Quiensoy<<< dim3(2,2), dim3(2,1) >>>();\n",
            "Soy el thread (0,0,0) del bloque (1,0,0) [blockDim=(2,1,1),gridDim=(2,2,1)] \n",
            "Soy el thread (1,0,0) del bloque (1,0,0) [blockDim=(2,1,1),gridDim=(2,2,1)] \n",
            "Soy el thread (0,0,0) del bloque (0,1,0) [blockDim=(2,1,1),gridDim=(2,2,1)] \n",
            "Soy el thread (1,0,0) del bloque (0,1,0) [blockDim=(2,1,1),gridDim=(2,2,1)] \n",
            "Soy el thread (0,0,0) del bloque (0,0,0) [blockDim=(2,1,1),gridDim=(2,2,1)] \n",
            "Soy el thread (1,0,0) del bloque (0,0,0) [blockDim=(2,1,1),gridDim=(2,2,1)] \n",
            "Soy el thread (0,0,0) del bloque (1,1,0) [blockDim=(2,1,1),gridDim=(2,2,1)] \n",
            "Soy el thread (1,0,0) del bloque (1,1,0) [blockDim=(2,1,1),gridDim=(2,2,1)] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./grillas 10 1 1 1 1 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RSk0qp9AyvB",
        "outputId": "17f3a9f7-faa2-4574-ddb9-1f4662547ee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Desde del host lanzamos\n",
            " Quiensoy<<< dim3(10,1,1), dim3(1,1,1) >>>():\n",
            "\n",
            "Y los hilos imprimen:\n",
            "Soy el thread (0,0,0) del bloque (0,0,0) [blockDim=(10,1,1),gridDim=(1,1,1)] \n",
            "Soy el thread (1,0,0) del bloque (0,0,0) [blockDim=(10,1,1),gridDim=(1,1,1)] \n",
            "Soy el thread (2,0,0) del bloque (0,0,0) [blockDim=(10,1,1),gridDim=(1,1,1)] \n",
            "Soy el thread (3,0,0) del bloque (0,0,0) [blockDim=(10,1,1),gridDim=(1,1,1)] \n",
            "Soy el thread (4,0,0) del bloque (0,0,0) [blockDim=(10,1,1),gridDim=(1,1,1)] \n",
            "Soy el thread (5,0,0) del bloque (0,0,0) [blockDim=(10,1,1),gridDim=(1,1,1)] \n",
            "Soy el thread (6,0,0) del bloque (0,0,0) [blockDim=(10,1,1),gridDim=(1,1,1)] \n",
            "Soy el thread (7,0,0) del bloque (0,0,0) [blockDim=(10,1,1),gridDim=(1,1,1)] \n",
            "Soy el thread (8,0,0) del bloque (0,0,0) [blockDim=(10,1,1),gridDim=(1,1,1)] \n",
            "Soy el thread (9,0,0) del bloque (0,0,0) [blockDim=(10,1,1),gridDim=(1,1,1)] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicios:\n",
        "\n",
        "* Experimentar con distintos números.\n",
        "* ¿Qué pasa si el número de hilos por bloque es mayor a 1024?\n",
        "* ¿Cual es el máximo número de bloques?"
      ],
      "metadata": {
        "id": "f3IMM-is6jXY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Atrapando Errores\n",
        "\n"
      ],
      "metadata": {
        "id": "FY8_Otx3KgO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile estefalla.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "\n",
        "\n",
        "// CUDA error checking macro\n",
        "#define CUDA_CHECK(call)                                      \\\n",
        "    {                                                        \\\n",
        "        cudaError_t err = call;                              \\\n",
        "        if (err != cudaSuccess) {                            \\\n",
        "            fprintf(stderr, \"CUDA Error: %s (at %s:%d)\\n\",   \\\n",
        "                    cudaGetErrorString(err), __FILE__, __LINE__); \\\n",
        "            exit(EXIT_FAILURE);                              \\\n",
        "        }                                                    \\\n",
        "    }\n",
        "\n",
        "// Failing CUDA Kernel (Out-of-Bounds Write)\n",
        "__global__ void faultyKernel(int *d_array) {\n",
        "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    // Intentional out-of-bounds access\n",
        "    d_array[idx] = 42;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int *d_array;\n",
        "    int size = 10 * sizeof(int);  // Allocate space for 10 integers\n",
        "\n",
        "    // **1. Intentional Memory Allocation Failure**\n",
        "    CUDA_CHECK(cudaMalloc((void**)&d_array, size * 100000000)); // Too much memory!\n",
        "\n",
        "    // **2. Launch Kernel with Too Many Threads**\n",
        "    dim3 threadsPerBlock(2048);  // Exceeds max threads per block\n",
        "    dim3 numBlocks(1);\n",
        "\n",
        "    faultyKernel<<<numBlocks, threadsPerBlock>>>(d_array);\n",
        "\n",
        "    // **3. Check for Kernel Launch Errors**\n",
        "    cudaError_t err = cudaGetLastError();\n",
        "    if (err != cudaSuccess) {\n",
        "        fprintf(stderr, \"Kernel launch failed: %s\\n\", cudaGetErrorString(err));\n",
        "    }\n",
        "\n",
        "    // **4. Synchronize to Catch Runtime Errors**\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    // Free memory (won't reach this point due to failures)\n",
        "    CUDA_CHECK(cudaFree(d_array));\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hhs-2qOLuHQ",
        "outputId": "f527366f-bd58-4fb6-e5ad-5825f8202247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting estefalla.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 estefalla.cu -o estefalla"
      ],
      "metadata": {
        "id": "wFGF0QjVLzzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./estefalla"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3OJGNuNL2i9",
        "outputId": "4d00b252-929e-4c27-cf33-237e4fe3c728"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Error: out of memory (at estefalla.cu:30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CUDA Samples\n",
        "\n",
        "Para aprender CUDA, este repositorio es ideal.\n",
        "\n"
      ],
      "metadata": {
        "id": "tYCUy0emaMnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NVIDIA/cuda-samples.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDp-X3Hj4X0I",
        "outputId": "6fc62a4e-7572-4c1f-a2ec-3caae5ae8448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cuda-samples'...\n",
            "remote: Enumerating objects: 19507, done.\u001b[K\n",
            "remote: Counting objects: 100% (4370/4370), done.\u001b[K\n",
            "remote: Compressing objects: 100% (752/752), done.\u001b[K\n",
            "remote: Total 19507 (delta 4059), reused 3618 (delta 3618), pack-reused 15137 (from 2)\u001b[K\n",
            "Receiving objects: 100% (19507/19507), 133.52 MiB | 14.16 MiB/s, done.\n",
            "Resolving deltas: 100% (17186/17186), done.\n",
            "Updating files: 100% (4026/4026), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls cuda-samples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzYognchaWOX",
        "outputId": "200c12b1-a707-4205-a6e0-525dfb4de18e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bin\t      Common   Makefile   Samples\t      Samples_VS2019.sln\n",
            "CHANGELOG.md  LICENSE  README.md  Samples_VS2017.sln  Samples_VS2022.sln\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/cuda-samples/Samples/1_Utilities/deviceQuery; make clean; make"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9HCDvXUarOY",
        "outputId": "d67e1484-d582-4bca-d2e3-1ae6e6d4da28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm -f deviceQuery deviceQuery.o\n",
            "rm -rf ../../../bin/x86_64/linux/release/deviceQuery\n",
            "/usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 --threads 0 --std=c++11 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o deviceQuery.o -c deviceQuery.cpp\n",
            "/usr/local/cuda/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o deviceQuery deviceQuery.o \n",
            "mkdir -p ../../../bin/x86_64/linux/release\n",
            "cp deviceQuery ../../../bin/x86_64/linux/release\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./cuda-samples/bin/x86_64/linux/release/deviceQuery"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWPGiV2QazOL",
        "outputId": "dfe0210f-3f83-4e13-e672-889b4bf4acc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./cuda-samples/bin/x86_64/linux/release/deviceQuery Starting...\n",
            "\n",
            " CUDA Device Query (Runtime API) version (CUDART static linking)\n",
            "\n",
            "Detected 1 CUDA Capable device(s)\n",
            "\n",
            "Device 0: \"Tesla T4\"\n",
            "  CUDA Driver Version / Runtime Version          12.4 / 12.5\n",
            "  CUDA Capability Major/Minor version number:    7.5\n",
            "  Total amount of global memory:                 15095 MBytes (15828320256 bytes)\n",
            "  (040) Multiprocessors, (064) CUDA Cores/MP:    2560 CUDA Cores\n",
            "  GPU Max Clock rate:                            1590 MHz (1.59 GHz)\n",
            "  Memory Clock rate:                             5001 Mhz\n",
            "  Memory Bus Width:                              256-bit\n",
            "  L2 Cache Size:                                 4194304 bytes\n",
            "  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\n",
            "  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\n",
            "  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\n",
            "  Total amount of constant memory:               65536 bytes\n",
            "  Total amount of shared memory per block:       49152 bytes\n",
            "  Total shared memory per multiprocessor:        65536 bytes\n",
            "  Total number of registers available per block: 65536\n",
            "  Warp size:                                     32\n",
            "  Maximum number of threads per multiprocessor:  1024\n",
            "  Maximum number of threads per block:           1024\n",
            "  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n",
            "  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n",
            "  Maximum memory pitch:                          2147483647 bytes\n",
            "  Texture alignment:                             512 bytes\n",
            "  Concurrent copy and kernel execution:          Yes with 3 copy engine(s)\n",
            "  Run time limit on kernels:                     No\n",
            "  Integrated GPU sharing Host Memory:            No\n",
            "  Support host page-locked memory mapping:       Yes\n",
            "  Alignment requirement for Surfaces:            Yes\n",
            "  Device has ECC support:                        Enabled\n",
            "  Device supports Unified Addressing (UVA):      Yes\n",
            "  Device supports Managed Memory:                Yes\n",
            "  Device supports Compute Preemption:            Yes\n",
            "  Supports Cooperative Kernel Launch:            Yes\n",
            "  Supports MultiDevice Co-op Kernel Launch:      Yes\n",
            "  Device PCI Domain ID / Bus ID / location ID:   0 / 0 / 4\n",
            "  Compute Mode:\n",
            "     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n",
            "\n",
            "deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 12.4, CUDA Runtime Version = 12.5, NumDevs = 1\n",
            "Result = PASS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-IuWUIUXaz51"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}